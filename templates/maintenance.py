import streamlit as st
import requests
import time
import json
from datetime import datetime
import sys
import os

# æ·»åŠ utilsç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from utils.api_config import get_files_api_url


def show_maintenance():
    """æ˜¾ç¤ºç»´æŠ¤ç®¡ç†é¡µé¢"""
    st.title("ğŸ› ï¸ ç³»ç»Ÿç»´æŠ¤ç®¡ç†")

    st.markdown("""
    è¿™ä¸ªé¡µé¢æä¾›æ–‡æ¡£æ¸…ç†å’Œç»´æŠ¤åŠŸèƒ½ï¼Œå¸®åŠ©ä¿æŒçŸ¥è¯†åº“çš„æ•´æ´å’Œå‡†ç¡®æ€§ã€‚

    âš ï¸ **æ³¨æ„**: åˆ é™¤æ“ä½œä¸å¯é€†ï¼Œè¯·è°¨æ…ä½¿ç”¨ï¼
    """)

    # åˆ›å»ºä¸¤ä¸ªæ ‡ç­¾é¡µ
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“‹ ç‰ˆæœ¬å»é‡", "ğŸ—‘ï¸ è¿‡æœŸæ¸…ç†", "ğŸ“¥ æ•°æ®å¯¼å…¥", "ğŸ“Š ä»»åŠ¡ç›‘æ§"])

    with tab1:
        show_version_cleanup_tab()

    with tab2:
        show_expired_cleanup_tab()

    with tab3:
        show_data_import_tab()

    with tab4:
        show_task_monitor_tab()


def show_version_cleanup_tab():
    """æ˜¾ç¤ºç‰ˆæœ¬å»é‡æ ‡ç­¾é¡µ"""
    st.header("ğŸ”„ æ€»è¡Œå‘æ–‡ç‰ˆæœ¬å»é‡")

    st.markdown("""
    ### åŠŸèƒ½è¯´æ˜
    è‡ªåŠ¨æ£€æµ‹å’Œæ¸…ç†æ€»è¡Œå‘æ–‡ä¸­çš„æ—§ç‰ˆæœ¬æ–‡æ¡£ï¼š

    1. ğŸ” æ£€æµ‹æ–‡æ¡£åä¸­çš„ä¿®è®¢å…³é”®è¯ï¼ˆä¿®è®¢ã€ä¿®æ”¹ã€æ›´æ–°ã€åºŸæ­¢ç­‰ï¼‰
    2. ğŸ“ æå–ã€Šã€‹ä¸­çš„æ ‡é¢˜è¿›è¡Œæ¨¡ç³ŠåŒ¹é…
    3. ğŸ¤– ä½¿ç”¨AIåˆ†ææ–‡æ¡£ç‰ˆæœ¬å·ï¼ˆå¦‚ï¼šæ˜†å†œå•†å‘ã€2025ã€‘xxxå·ï¼‰
    4. ğŸ—‘ï¸ åˆ é™¤æ—§ç‰ˆæœ¬ï¼Œä¿ç•™æœ€æ–°ç‰ˆæœ¬

    ### å½“å‰è®¾ç½®
    """)

    col1, col2 = st.columns(2)

    with col1:
        st.info("**å®šæ—¶ä»»åŠ¡**: æ¯å¤©å‡Œæ™¨2ç‚¹è‡ªåŠ¨æ‰§è¡Œ")
        st.info("**é»˜è®¤å¤„ç†**: 50ä¸ªæ–‡æ¡£/æ¬¡")

    with col2:
        st.warning("**ä¿®è®¢å…³é”®è¯**")
        st.code("ä¿®è®¢ã€ä¿®æ”¹ã€æ›´æ–°ã€è°ƒæ•´ã€å˜æ›´\nä¿®æ­£ã€è¡¥å……ã€å®Œå–„ã€åºŸæ­¢ã€åºŸé™¤")

    st.divider()

    # æ‰‹åŠ¨è§¦å‘åŒºåŸŸ
    st.subheader("âš¡ æ‰‹åŠ¨è§¦å‘")

    col1, col2, col3 = st.columns([2, 1, 1])

    with col1:
        limit = st.slider(
            "å¤„ç†æ–‡æ¡£æ•°é‡",
            min_value=10,
            max_value=200,
            value=50,
            step=10,
            help="ä¸€æ¬¡æ€§å¤„ç†çš„æ–‡æ¡£æ•°é‡ï¼Œå»ºè®®å…ˆä»å°æ•°é‡å¼€å§‹æµ‹è¯•"
        )

    with col2:
        st.write("")  # å ä½
        st.write("")  # å ä½
        dry_run = st.checkbox("é¢„è§ˆæ¨¡å¼", value=False, help="ä¸å®é™…åˆ é™¤ï¼Œä»…æ˜¾ç¤ºä¼šåˆ é™¤çš„æ–‡æ¡£")

    with col3:
        st.write("")  # å ä½
        st.write("")  # å ä½

    if st.button("ğŸš€ å¼€å§‹æ¸…ç†", key="start_version_cleanup", type="primary", use_container_width=True):
        if dry_run:
            st.warning("âš ï¸ é¢„è§ˆæ¨¡å¼æš‚æœªå®ç°ï¼Œå°†ç›´æ¥æ‰§è¡Œæ¸…ç†æ“ä½œ")

        with st.spinner("æ­£åœ¨æäº¤æ¸…ç†ä»»åŠ¡..."):
            result = trigger_clean_version_duplicates(limit)

            if result.get('success'):
                st.success(f"âœ… {result.get('message')}")

                # æ˜¾ç¤ºä»»åŠ¡ä¿¡æ¯
                task_id = result.get('task_id')
                st.info(f"ğŸ“ **ä»»åŠ¡ID**: `{task_id}`")
                st.info(f"ğŸ“„ **å¤„ç†æ•°é‡**: {limit} ä¸ªæ–‡æ¡£")

                # ä¿å­˜åˆ°session stateç”¨äºç›‘æ§
                if 'running_tasks' not in st.session_state:
                    st.session_state.running_tasks = []

                st.session_state.running_tasks.append({
                    'task_id': task_id,
                    'type': 'version_cleanup',
                    'limit': limit,
                    'started_at': datetime.now().isoformat(),
                    'status': 'running'
                })

                st.success("ğŸ’¡ å¯ä»¥åœ¨ã€ä»»åŠ¡ç›‘æ§ã€‘æ ‡ç­¾é¡µæŸ¥çœ‹æ‰§è¡Œè¿›åº¦")
            else:
                st.error(f"âŒ ä»»åŠ¡æäº¤å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")


def show_expired_cleanup_tab():
    """æ˜¾ç¤ºè¿‡æœŸæ¸…ç†æ ‡ç­¾é¡µ"""
    st.header("ğŸ—‘ï¸ è¿‡æœŸæ–‡æ¡£æ¸…ç†")

    st.markdown("""
    ### åŠŸèƒ½è¯´æ˜
    è‡ªåŠ¨æ£€æŸ¥å’Œæ¸…ç†è¿‡æœŸæ–‡æ¡£ï¼š

    1. ğŸ“… æ£€æŸ¥ai_metadataä¸­çš„expiration_dateå­—æ®µ
    2. â° å¯¹æ¯”å½“å‰æ—¥æœŸåˆ¤æ–­æ˜¯å¦è¿‡æœŸ
    3. ğŸ¤– å¦‚æœæ²¡æœ‰å…ƒæ•°æ®ï¼Œä½¿ç”¨AIåˆ¤æ–­æœ‰æ•ˆæœŸ
    4. ğŸ—‘ï¸ åˆ é™¤å·²è¿‡æœŸçš„æ–‡æ¡£

    ### å½“å‰è®¾ç½®
    """)

    col1, col2 = st.columns(2)

    with col1:
        st.info("**å®šæ—¶ä»»åŠ¡**: æ¯å‘¨æ—¥å‡Œæ™¨3ç‚¹è‡ªåŠ¨æ‰§è¡Œï¼ˆæ¯7å¤©ä¸€æ¬¡ï¼‰")
        st.info("**é»˜è®¤å¤„ç†**: 50ä¸ªæ–‡æ¡£/æ¬¡")

    with col2:
        st.success("**æ°¸ä¹…æœ‰æ•ˆæ ‡è¯†**")
        st.code("æ°¸ä¹…ã€æ— ã€permanent\nnoneã€neverã€é•¿æœŸ")

    st.divider()

    # æ‰‹åŠ¨è§¦å‘åŒºåŸŸ
    st.subheader("âš¡ æ‰‹åŠ¨è§¦å‘")

    col1, col2, col3 = st.columns([2, 1, 1])

    with col1:
        limit = st.slider(
            "å¤„ç†æ–‡æ¡£æ•°é‡",
            min_value=10,
            max_value=5000,
            value=50,
            step=10,
            help="ä¸€æ¬¡æ€§å¤„ç†çš„æ–‡æ¡£æ•°é‡",
            key="expired_limit"
        )

    with col2:
        st.write("")  # å ä½
        st.write("")  # å ä½
        dry_run = st.checkbox("é¢„è§ˆæ¨¡å¼", value=False, help="ä¸å®é™…åˆ é™¤ï¼Œä»…æ˜¾ç¤ºä¼šåˆ é™¤çš„æ–‡æ¡£", key="expired_dry_run")

    with col3:
        st.write("")  # å ä½
        st.write("")  # å ä½

    if st.button("ğŸš€ å¼€å§‹æ¸…ç†", key="start_expired_cleanup", type="primary", use_container_width=True):
        if dry_run:
            st.warning("âš ï¸ é¢„è§ˆæ¨¡å¼æš‚æœªå®ç°ï¼Œå°†ç›´æ¥æ‰§è¡Œæ¸…ç†æ“ä½œ")

        with st.spinner("æ­£åœ¨æäº¤æ¸…ç†ä»»åŠ¡..."):
            result = trigger_clean_expired_documents(limit)

            if result.get('success'):
                st.success(f"âœ… {result.get('message')}")

                # æ˜¾ç¤ºä»»åŠ¡ä¿¡æ¯
                task_id = result.get('task_id')
                st.info(f"ğŸ“ **ä»»åŠ¡ID**: `{task_id}`")
                st.info(f"ğŸ“„ **å¤„ç†æ•°é‡**: {limit} ä¸ªæ–‡æ¡£")

                # ä¿å­˜åˆ°session stateç”¨äºç›‘æ§
                if 'running_tasks' not in st.session_state:
                    st.session_state.running_tasks = []

                st.session_state.running_tasks.append({
                    'task_id': task_id,
                    'type': 'expired_cleanup',
                    'limit': limit,
                    'started_at': datetime.now().isoformat(),
                    'status': 'running'
                })

                st.success("ğŸ’¡ å¯ä»¥åœ¨ã€ä»»åŠ¡ç›‘æ§ã€‘æ ‡ç­¾é¡µæŸ¥çœ‹æ‰§è¡Œè¿›åº¦")
            else:
                st.error(f"âŒ ä»»åŠ¡æäº¤å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")


def show_data_import_tab():
    """æ˜¾ç¤ºæ•°æ®å¯¼å…¥æ ‡ç­¾é¡µ"""
    st.header("ğŸ“¥ DATæ–‡ä»¶æ•°æ®å¯¼å…¥")

    st.markdown("""
    ### åŠŸèƒ½è¯´æ˜
    ä»æ•°æ®ç»„æä¾›çš„.datæ–‡ä»¶ä¸­å¯¼å…¥æ–‡ä»¶ä¿¡æ¯ï¼š

    1. ğŸ“„ è¯»å–DATæ–‡ä»¶ï¼ˆä½¿ç”¨ASCIIç 1ä½œä¸ºå­—æ®µåˆ†éš”ç¬¦ï¼‰
    2. ğŸ” æ£€æµ‹å·²å­˜åœ¨çš„è®°å½•
    3. â• å¢é‡å¯¼å…¥æ–°è®°å½•
    4. â™»ï¸ å¯é€‰æ‹©æ›´æ–°å·²å­˜åœ¨çš„è®°å½•

    ### å½“å‰è®¾ç½®
    """)

    col1, col2 = st.columns(2)

    with col1:
        st.info("**å®šæ—¶ä»»åŠ¡**: æ¯å¤©å‡Œæ™¨2:10è‡ªåŠ¨æ‰§è¡Œ")
        st.info("**å¯¼å…¥ç›®å½•**: `/data/dat_files`")

    with col2:
        st.success("**å­—æ®µåˆ†éš”ç¬¦**: ASCIIç 1ï¼ˆ`\\x01`ï¼‰")
        st.success("**å¯¼å…¥æ¨¡å¼**: å¢é‡å¯¼å…¥ï¼ˆé»˜è®¤è·³è¿‡å·²å­˜åœ¨ï¼‰")

    st.divider()

    # æŸ¥è¯¢å¯¼å…¥çŠ¶æ€
    st.subheader("ğŸ“Š å¯¼å…¥å†å²")

    with st.spinner("æ­£åœ¨åŠ è½½å¯¼å…¥ç»Ÿè®¡..."):
        import_status = get_import_status()

        if import_status:
            col1, col2 = st.columns(2)

            with col1:
                st.metric("æ€»å¯¼å…¥è®°å½•æ•°", import_status.get('total_imported', 0))

            with col2:
                recent = import_status.get('recent_imports', [])
                if recent:
                    last_import = recent[0]
                    st.metric("æœ€è¿‘å¯¼å…¥æ—¥æœŸ", last_import.get('date', 'N/A'))
                else:
                    st.metric("æœ€è¿‘å¯¼å…¥æ—¥æœŸ", "æš‚æ— è®°å½•")

            # æ˜¾ç¤ºæœ€è¿‘çš„å¯¼å…¥è®°å½•
            if recent:
                st.markdown("#### æœ€è¿‘10æ¬¡å¯¼å…¥")
                import pandas as pd

                df = pd.DataFrame(recent)
                df.columns = ['æ—¥æœŸ', 'æ¥æº', 'æ•°é‡']
                st.dataframe(df, use_container_width=True, hide_index=True)

    st.divider()

    # æ‰‹åŠ¨è§¦å‘åŒºåŸŸ
    st.subheader("âš¡ æ‰‹åŠ¨è§¦å‘å¯¼å…¥")

    col1, col2 = st.columns([3, 1])

    with col1:
        dat_file_path = st.text_input(
            "DATæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰",
            placeholder="ç•™ç©ºåˆ™è‡ªåŠ¨é€‰æ‹©æœ€æ–°æ–‡ä»¶",
            help="è¾“å…¥å®Œæ•´çš„DATæ–‡ä»¶è·¯å¾„ï¼Œæˆ–ç•™ç©ºè®©ç³»ç»Ÿè‡ªåŠ¨é€‰æ‹©æœ€æ–°çš„æ–‡ä»¶"
        )

    with col2:
        st.write("")  # å ä½
        st.write("")  # å ä½
        update_existing = st.checkbox(
            "æ›´æ–°å·²å­˜åœ¨è®°å½•",
            value=False,
            help="å‹¾é€‰åˆ™æ›´æ–°å·²å­˜åœ¨çš„è®°å½•ï¼Œä¸å‹¾é€‰åˆ™è·³è¿‡å·²å­˜åœ¨çš„è®°å½•"
        )

    col1, col2, col3 = st.columns([1, 1, 2])

    with col1:
        if st.button("ğŸš€ å¼€å§‹å¯¼å…¥", key="start_import", type="primary", use_container_width=True):
            with st.spinner("æ­£åœ¨æäº¤å¯¼å…¥ä»»åŠ¡..."):
                # å‡†å¤‡è¯·æ±‚å‚æ•°
                import_params = {}
                if dat_file_path.strip():
                    import_params['dat_file_path'] = dat_file_path.strip()
                import_params['update_existing'] = update_existing

                result = trigger_import_dat_file(import_params)

                if result.get('success'):
                    st.success(f"âœ… {result.get('message')}")

                    # æ˜¾ç¤ºä»»åŠ¡ä¿¡æ¯
                    task_id = result.get('task_id')
                    st.info(f"ğŸ“ **ä»»åŠ¡ID**: `{task_id}`")
                    st.info(f"ğŸ“„ **æ›´æ–°æ¨¡å¼**: {'æ˜¯' if update_existing else 'å¦'}")

                    # ä¿å­˜åˆ°session stateç”¨äºç›‘æ§
                    if 'running_tasks' not in st.session_state:
                        st.session_state.running_tasks = []

                    st.session_state.running_tasks.append({
                        'task_id': task_id,
                        'type': 'dat_import',
                        'dat_file_path': dat_file_path or 'è‡ªåŠ¨é€‰æ‹©',
                        'update_existing': update_existing,
                        'started_at': datetime.now().isoformat(),
                        'status': 'running'
                    })

                    st.success("ğŸ’¡ å¯ä»¥åœ¨ã€ä»»åŠ¡ç›‘æ§ã€‘æ ‡ç­¾é¡µæŸ¥çœ‹æ‰§è¡Œè¿›åº¦")
                else:
                    st.error(f"âŒ ä»»åŠ¡æäº¤å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

    with col2:
        if st.button("ğŸ”„ åˆ·æ–°ç»Ÿè®¡", key="refresh_import_stats", use_container_width=True):
            st.rerun()


def show_task_monitor_tab():
    """æ˜¾ç¤ºä»»åŠ¡ç›‘æ§æ ‡ç­¾é¡µ"""
    st.header("ğŸ“Š ä»»åŠ¡ç›‘æ§")

    # åˆ·æ–°æŒ‰é’®
    col1, col2, col3 = st.columns([1, 1, 3])
    with col1:
        if st.button("ğŸ”„ åˆ·æ–°çŠ¶æ€", key="refresh_tasks"):
            st.rerun()

    with col2:
        if st.button("ğŸ—‘ï¸ æ¸…ç©ºå†å²", key="clear_tasks"):
            st.session_state.running_tasks = []
            st.rerun()

    st.divider()

    # æ˜¾ç¤ºè¿è¡Œä¸­çš„ä»»åŠ¡
    if 'running_tasks' not in st.session_state or not st.session_state.running_tasks:
        st.info("ğŸ“­ æš‚æ— è¿è¡Œä¸­çš„ä»»åŠ¡")
        st.markdown("""
        ### ğŸ’¡ æç¤º
        - åœ¨ã€ç‰ˆæœ¬å»é‡ã€‘æˆ–ã€è¿‡æœŸæ¸…ç†ã€‘æ ‡ç­¾é¡µæäº¤ä»»åŠ¡åï¼Œä¼šåœ¨è¿™é‡Œæ˜¾ç¤º
        - å¯ä»¥å®æ—¶æŸ¥çœ‹ä»»åŠ¡æ‰§è¡ŒçŠ¶æ€å’Œç»“æœ
        """)
        return

    st.success(f"ğŸ“‹ å…±æœ‰ {len(st.session_state.running_tasks)} ä¸ªä»»åŠ¡")

    # æ˜¾ç¤ºæ¯ä¸ªä»»åŠ¡çš„çŠ¶æ€
    for idx, task_info in enumerate(st.session_state.running_tasks):
        task_id = task_info['task_id']
        task_type = task_info['type']
        started_at = task_info['started_at']

        # ä»»åŠ¡ç±»å‹æ˜¾ç¤º
        if task_type == 'version_cleanup':
            task_type_name = "ğŸ”„ ç‰ˆæœ¬å»é‡"
        elif task_type == 'expired_cleanup':
            task_type_name = "ğŸ—‘ï¸ è¿‡æœŸæ¸…ç†"
        elif task_type == 'dat_import':
            task_type_name = "ğŸ“¥ æ•°æ®å¯¼å…¥"
        else:
            task_type_name = "â“ æœªçŸ¥ä»»åŠ¡"

        with st.expander(f"{task_type_name} - {task_id[:12]}...", expanded=(idx == len(st.session_state.running_tasks) - 1)):
            # æ ¹æ®ä»»åŠ¡ç±»å‹æ˜¾ç¤ºä¸åŒçš„ä¿¡æ¯
            if task_type == 'dat_import':
                st.markdown(f"""
                **ä»»åŠ¡ID**: `{task_id}`
                **ä»»åŠ¡ç±»å‹**: {task_type_name}
                **DATæ–‡ä»¶è·¯å¾„**: {task_info.get('dat_file_path', 'è‡ªåŠ¨é€‰æ‹©')}
                **æ›´æ–°å·²å­˜åœ¨è®°å½•**: {'æ˜¯' if task_info.get('update_existing', False) else 'å¦'}
                **å¼€å§‹æ—¶é—´**: {started_at}
                """)
            else:
                st.markdown(f"""
                **ä»»åŠ¡ID**: `{task_id}`
                **ä»»åŠ¡ç±»å‹**: {task_type_name}
                **å¤„ç†æ•°é‡**: {task_info.get('limit', 'N/A')} ä¸ªæ–‡æ¡£
                **å¼€å§‹æ—¶é—´**: {started_at}
                """)

            # æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€
            status_result = check_task_status(task_id)

            if status_result:
                state = status_result.get('state', 'UNKNOWN')
                ready = status_result.get('ready', False)
                successful = status_result.get('successful')

                # æ˜¾ç¤ºçŠ¶æ€
                col1, col2 = st.columns(2)

                with col1:
                    if state == 'PENDING':
                        st.warning("â³ ç­‰å¾…æ‰§è¡Œ")
                    elif state == 'PROGRESS':
                        st.info("ğŸ”„ æ‰§è¡Œä¸­...")
                    elif state == 'SUCCESS':
                        st.success("âœ… æ‰§è¡ŒæˆåŠŸ")
                    elif state == 'FAILURE':
                        st.error("âŒ æ‰§è¡Œå¤±è´¥")
                    else:
                        st.warning(f"â“ æœªçŸ¥çŠ¶æ€: {state}")

                with col2:
                    if ready:
                        st.metric("çŠ¶æ€", "å·²å®Œæˆ" if successful else "å¤±è´¥")
                    else:
                        st.metric("çŠ¶æ€", "è¿›è¡Œä¸­")

                # æ˜¾ç¤ºè¯¦ç»†ç»“æœ
                if ready and successful:
                    result = status_result.get('result', {})

                    st.success("### ğŸ“ˆ æ‰§è¡Œç»“æœ")

                    # DATå¯¼å…¥ä»»åŠ¡çš„ç»“æœå±•ç¤º
                    if task_type == 'dat_import':
                        stats = result.get('stats', {})

                        col1, col2, col3, col4 = st.columns(4)

                        with col1:
                            st.metric("æ€»è¡Œæ•°", stats.get('total_lines', 0))

                        with col2:
                            st.metric("æ–°å¢è®°å½•", stats.get('new_records', 0))

                        with col3:
                            st.metric("æ›´æ–°è®°å½•", stats.get('updated_records', 0))

                        with col4:
                            st.metric("é”™è¯¯æ•°", stats.get('error_records', 0))

                        col1, col2 = st.columns(2)

                        with col1:
                            st.metric("è§£ææˆåŠŸ", stats.get('parsed_lines', 0))

                        with col2:
                            st.metric("è·³è¿‡è®°å½•", stats.get('skipped_records', 0))

                        # æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
                        errors = stats.get('errors', [])
                        if errors:
                            st.markdown("### âš ï¸ é”™è¯¯ä¿¡æ¯")
                            for error in errors[:5]:  # åªæ˜¾ç¤ºå‰5æ¡
                                st.warning(error)

                            if len(errors) > 5:
                                st.caption(f"... è¿˜æœ‰ {len(errors) - 5} æ¡é”™è¯¯æœªæ˜¾ç¤º")

                    # å…¶ä»–ä»»åŠ¡çš„ç»“æœå±•ç¤º
                    else:
                        col1, col2, col3, col4 = st.columns(4)

                        with col1:
                            st.metric("å¤„ç†æ–‡æ¡£", result.get('processed', 0))

                        with col2:
                            if task_type == 'version_cleanup':
                                st.metric("å‘ç°é‡å¤", result.get('duplicates_found', 0))
                            else:
                                st.metric("å…ƒæ•°æ®è¿‡æœŸ", result.get('expired_by_metadata', 0))

                        with col3:
                            if task_type == 'version_cleanup':
                                st.metric("åˆ é™¤æ–‡æ¡£", result.get('deleted', 0))
                            else:
                                st.metric("AIåˆ¤å®šè¿‡æœŸ", result.get('expired_by_ai', 0))

                        with col4:
                            st.metric("é”™è¯¯æ•°", result.get('errors', 0))

                        # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
                        details = result.get('details', [])
                        if details:
                            st.markdown("### ğŸ“‹ è¯¦ç»†ä¿¡æ¯")
                            for detail in details[:5]:  # åªæ˜¾ç¤ºå‰5æ¡
                                if task_type == 'version_cleanup':
                                    st.info(f"""
                                    **æ ‡é¢˜**: {detail.get('title')}
                                    **æœ€æ–°ç‰ˆæœ¬**: {detail.get('latest_document')}
                                    **åˆ é™¤æ•°é‡**: {detail.get('deleted_count')}
                                    **åˆ¤æ–­ç†ç”±**: {detail.get('reasoning')}
                                    """)
                                else:
                                    st.info(f"""
                                    **æ–‡ä»¶å**: {detail.get('filename')}
                                    **æ£€æŸ¥æ–¹å¼**: {detail.get('check_method')}
                                    **è¿‡æœŸæ—¥æœŸ**: {detail.get('expiration_date', 'N/A')}
                                    **åˆ¤æ–­ç†ç”±**: {detail.get('reasoning', 'N/A')}
                                    """)

                            if len(details) > 5:
                                st.caption(f"... è¿˜æœ‰ {len(details) - 5} æ¡è®°å½•æœªæ˜¾ç¤º")

                elif ready and not successful:
                    error = status_result.get('error', 'æœªçŸ¥é”™è¯¯')
                    st.error(f"### âŒ æ‰§è¡Œé”™è¯¯\n{error}")

                else:
                    info = status_result.get('info', 'ä»»åŠ¡æ­£åœ¨æ‰§è¡Œä¸­...')
                    st.info(f"ğŸ’¬ {info}")
            else:
                st.error("âŒ æ— æ³•æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€")


# APIè°ƒç”¨å‡½æ•°
def trigger_clean_version_duplicates(limit=50):
    """è§¦å‘ç‰ˆæœ¬å»é‡æ¸…ç†"""
    try:
        base_url = get_files_api_url("").rstrip('/files/')
        url = f"{base_url}/maintenance/clean-version-duplicates?limit={limit}"
        response = requests.post(url, timeout=10)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        return {'success': False, 'error': f'APIè°ƒç”¨å¤±è´¥: {str(e)}'}
    except Exception as e:
        return {'success': False, 'error': str(e)}


def trigger_clean_expired_documents(limit=50):
    """è§¦å‘è¿‡æœŸæ–‡æ¡£æ¸…ç†"""
    try:
        base_url = get_files_api_url("").rstrip('/files/')
        url = f"{base_url}/maintenance/clean-expired-documents?limit={limit}"
        response = requests.post(url, timeout=10)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        return {'success': False, 'error': f'APIè°ƒç”¨å¤±è´¥: {str(e)}'}
    except Exception as e:
        return {'success': False, 'error': str(e)}


def check_task_status(task_id):
    """æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€"""
    try:
        base_url = get_files_api_url("").rstrip('/files/')
        url = f"{base_url}/maintenance/task-status/{task_id}"
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        st.error(f"æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€å¤±è´¥: {str(e)}")
        return None
    except Exception as e:
        st.error(f"æŸ¥è¯¢å¤±è´¥: {str(e)}")
        return None


def trigger_import_dat_file(params):
    """è§¦å‘DATæ–‡ä»¶å¯¼å…¥"""
    try:
        base_url = get_files_api_url("").rstrip('/files/')
        url = f"{base_url}/data/import-dat"
        response = requests.post(url, json=params, timeout=10)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        return {'success': False, 'error': f'APIè°ƒç”¨å¤±è´¥: {str(e)}'}
    except Exception as e:
        return {'success': False, 'error': str(e)}


def get_import_status():
    """è·å–å¯¼å…¥çŠ¶æ€ç»Ÿè®¡"""
    try:
        base_url = get_files_api_url("").rstrip('/files/')
        url = f"{base_url}/data/import-status"
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        st.error(f"è·å–å¯¼å…¥çŠ¶æ€å¤±è´¥: {str(e)}")
        return None
    except Exception as e:
        st.error(f"è·å–å¤±è´¥: {str(e)}")
        return None
